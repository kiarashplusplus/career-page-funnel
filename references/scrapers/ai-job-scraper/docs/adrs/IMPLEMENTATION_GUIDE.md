# AI Job Scraper Implementation Guide

## Overview

This guide provides step-by-step implementation instructions for the AI Job Scraper based on the comprehensive Architecture Decision Records (ADR-001 through ADR-023). The system is designed for library-first implementation with a target of 1-week deployment and zero maintenance operation.

**Target**: Single-user job search management with <$50/month operational costs and <1,000 job capacity.

## Quick Start

### Prerequisites

- **Python 3.12+**: Required for sys.monitoring performance improvements (ADR-001)
- **Standard Laptop/Desktop**: 8GB RAM, 2GB free disk space (no GPU required)
- **Docker & docker-compose**: For development environment consistency (ADR-023)
- **Internet Connection**: For proxy services and cloud AI fallback

### Rapid Deployment

```bash
# 1. Clone and setup
git clone https://github.com/yourusername/ai-job-scraper.git
cd ai-job-scraper

# 2. Environment setup (ADR-002)
uv sync                              # Install all dependencies
cp .env.example .env                 # Configure environment
# Edit .env with your API keys

# 3. Start development environment (ADR-023)
./scripts/dev-start.sh               # Starts Docker containers with hot reload

# 4. Verify installation
curl http://localhost:8000/health    # Check backend health
curl http://localhost:3000           # Check frontend accessibility
```

## Phase-Based Implementation Strategy

### Phase 1: Foundation & Database (Week 1, Days 1-2)

**Implements**: ADR-001 (Library-First), ADR-005 (Database), ADR-007 (Service Layer)

#### 1.1 Database Foundation Setup

```bash
# Database initialization (ADR-005)
uv run python src/database/init_database.py
```

**File**: `src/models/database.py`

```python
from sqlmodel import SQLModel, Field, create_engine, Session
from datetime import datetime
from enum import Enum
from typing import Optional

class ApplicationStatus(str, Enum):
    """Application workflow status (ADR-020)"""
    NEW = "new"
    INTERESTED = "interested"  
    APPLIED = "applied"
    REJECTED = "rejected"

class JobModel(SQLModel, table=True):
    """Core job data model with status tracking"""
    __tablename__ = "jobs"
    
    # Core fields
    id: Optional[int] = Field(default=None, primary_key=True)
    title: str = Field(min_length=1, max_length=500)
    company: str = Field(min_length=1, max_length=200)
    location: Optional[str] = Field(default=None, max_length=200)
    description: Optional[str] = Field(default=None, max_length=10000)
    url: str = Field(unique=True, max_length=2000)
    
    # Timestamps
    scraped_at: datetime = Field(default_factory=datetime.now)
    
    # Application status tracking (ADR-020)
    application_status: ApplicationStatus = Field(default=ApplicationStatus.NEW)
    applied_at: Optional[datetime] = Field(default=None)
    status_updated_at: datetime = Field(default_factory=datetime.now)

# Database connection with WAL mode (ADR-005)
database_url = "sqlite:///./data/jobs.db"
engine = create_engine(database_url, connect_args={"check_same_thread": False})

def init_database():
    """Initialize database with FTS5 search (ADR-018)"""
    SQLModel.metadata.create_all(engine)
    
    with Session(engine) as session:
        # Enable FTS5 search with automatic triggers
        session.exec(text("""
            CREATE VIRTUAL TABLE IF NOT EXISTS jobs_fts USING fts5(
                title, company, location, description, 
                content='jobs', 
                tokenize='porter'
            )
        """))
        
        # Auto-update triggers for search index
        session.exec(text("""
            CREATE TRIGGER IF NOT EXISTS jobs_ai AFTER INSERT ON jobs BEGIN
                INSERT INTO jobs_fts(rowid, title, company, location, description)
                VALUES (new.id, new.title, new.company, new.location, new.description);
            END
        """))
        
        session.commit()
        print("‚úÖ Database and FTS5 search initialized")
```

#### 1.2 Service Layer Implementation

**File**: `src/services/base_service.py`

```python
from typing import Dict, Any, TypeVar, Generic, Optional
from sqlmodel import Session, select
from src.models.database import engine
import streamlit as st

T = TypeVar('T')

class BaseService(Generic[T]):
    """Base service with consistent error handling (ADR-007)"""
    
    def success_response(self, data: Any = None, message: str = "") -> Dict[str, Any]:
        """Standardized success response format"""
        return {
            "success": True,
            "data": data,
            "message": message,
            "error": None
        }
    
    def handle_service_error(self, operation: str, error: Exception) -> Dict[str, Any]:
        """Standardized error handling with logging"""
        error_msg = f"{operation} failed: {str(error)}"
        st.error(error_msg)  # User-friendly error display
        
        return {
            "success": False,
            "data": None,
            "message": "",
            "error": error_msg
        }

class JobService(BaseService[JobModel]):
    """Job management service with caching (ADR-007)"""
    
    @st.cache_data(ttl=300, show_spinner=False)
    def get_all_jobs(_self, limit: int = 1000) -> Dict[str, Any]:
        """Get all jobs with caching for performance"""
        try:
            with Session(engine) as session:
                jobs = session.exec(
                    select(JobModel)
                    .order_by(JobModel.scraped_at.desc())
                    .limit(limit)
                ).all()
                
                return _self.success_response(
                    data=[job.dict() for job in jobs],
                    message=f"Retrieved {len(jobs)} jobs"
                )
                
        except Exception as e:
            return _self.handle_service_error("get_all_jobs", e)
    
    def create_job(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create new job with validation"""
        try:
            with Session(engine) as session:
                job = JobModel(**job_data)
                session.add(job)
                session.commit()
                session.refresh(job)
                
                # Clear caches for immediate UI update
                st.cache_data.clear()
                
                return self.success_response(
                    data=job.dict(),
                    message=f"Created job: {job.title}"
                )
                
        except Exception as e:
            return self.handle_service_error("create_job", e)

# Global service instances
job_service = JobService()
```

### Phase 2: Search & Analytics (Week 1, Days 3-4)

**Implements**: ADR-018 (Search), ADR-019 (Analytics), ADR-020 (Status Tracking)

#### 2.1 Search Implementation

**File**: `src/services/search_service.py`

```python
from typing import List, Dict, Any
from sqlmodel import Session, text
from src.services.base_service import BaseService
from src.models.database import engine, ApplicationStatus
import streamlit as st

class JobSearchService(BaseService):
    """FTS5 search with status filtering (ADR-018, ADR-020)"""
    
    @st.cache_data(ttl=300, show_spinner=False)
    def search_jobs(_self, query: str, status_filter: List[ApplicationStatus] = None,
                   limit: int = 50) -> List[Dict[str, Any]]:
        """Search jobs with FTS5 and optional status filtering"""
        if not query.strip():
            return []
        
        try:
            with Session(engine) as session:
                # Build FTS5 query with status filtering
                base_query = """
                    SELECT jobs.*, jobs_fts.rank
                    FROM jobs_fts
                    JOIN jobs ON jobs.id = jobs_fts.rowid
                    WHERE jobs_fts MATCH ?
                """
                params = [query]
                
                # Add status filtering if specified
                if status_filter:
                    status_placeholders = ",".join(["?" for _ in status_filter])
                    base_query += f" AND jobs.application_status IN ({status_placeholders})"
                    params.extend([status.value for status in status_filter])
                
                # Order by relevance then recency
                base_query += " ORDER BY jobs_fts.rank DESC, jobs.status_updated_at DESC LIMIT ?"
                params.append(limit)
                
                results = list(session.exec(text(base_query), params))
                return [dict(row) for row in results]
                
        except Exception as e:
            st.error(f"Search error: {e}")
            return []
    
    @st.cache_data(ttl=60, show_spinner=False)
    def get_recent_jobs(_self, limit: int = 50) -> List[Dict[str, Any]]:
        """Get recent jobs for default display"""
        try:
            with Session(engine) as session:
                results = list(session.exec(text(
                    "SELECT * FROM jobs ORDER BY scraped_at DESC LIMIT ?"
                ), [limit]))
                return [dict(row) for row in results]
                
        except Exception as e:
            st.error(f"Recent jobs error: {e}")
            return []

# Global service instance
search_service = JobSearchService()
```

#### 2.2 Analytics with Intelligent Method Selection

**File**: `src/services/analytics_service.py`

```python
import time
import duckdb
from typing import Dict, Any, Optional
from sqlmodel import Session, text
from src.services.base_service import BaseService
from src.models.database import engine
import streamlit as st

class AnalyticsService(BaseService):
    """Analytics with intelligent SQLite/DuckDB selection (ADR-019)"""
    
    def __init__(self):
        self.performance_threshold_ms = 500  # Switch to DuckDB if slower
        self.last_query_time = 0
    
    @st.cache_data(ttl=300, show_spinner=False)
    def get_job_analytics(_self, use_duckdb: Optional[bool] = None) -> Dict[str, Any]:
        """Get comprehensive job analytics with intelligent method selection"""
        try:
            # Auto-select method based on performance history
            if use_duckdb is None:
                use_duckdb = _self.last_query_time > (_self.performance_threshold_ms / 1000)
            
            if use_duckdb:
                return _self._get_analytics_duckdb()
            else:
                start_time = time.perf_counter()
                result = _self._get_analytics_sqlite()
                _self.last_query_time = time.perf_counter() - start_time
                
                # Switch to DuckDB if performance degrades
                if _self.last_query_time > (_self.performance_threshold_ms / 1000):
                    st.info("Performance threshold exceeded, switching to DuckDB for analytics")
                    return _self._get_analytics_duckdb()
                
                return result
                
        except Exception as e:
            return _self.handle_service_error("get_job_analytics", e)
    
    def _get_analytics_sqlite(self) -> Dict[str, Any]:
        """SQLite-based analytics for standard performance"""
        with Session(engine) as session:
            # Status distribution
            status_counts = {}
            for status in ["new", "interested", "applied", "rejected"]:
                count = session.exec(text(
                    "SELECT COUNT(*) FROM jobs WHERE application_status = ?"
                ), [status]).one()
                status_counts[status] = count
            
            # Total jobs
            total_jobs = sum(status_counts.values())
            
            # Application funnel metrics
            applied_count = status_counts["applied"]
            rejected_count = status_counts["rejected"]
            
            application_rate = (applied_count / total_jobs * 100) if total_jobs > 0 else 0
            rejection_rate = (rejected_count / applied_count * 100) if applied_count > 0 else 0
            
            return self.success_response(data={
                "method": "SQLite",
                "total_jobs": total_jobs,
                "status_distribution": status_counts,
                "application_rate_percent": round(application_rate, 1),
                "rejection_rate_percent": round(rejection_rate, 1)
            })
    
    def _get_analytics_duckdb(self) -> Dict[str, Any]:
        """DuckDB-based analytics for high performance scenarios"""
        db_path = "./data/jobs.db"
        
        with duckdb.connect() as conn:
            # Install and load sqlite scanner
            conn.execute("INSTALL sqlite_scanner")
            conn.execute("LOAD sqlite_scanner")
            
            # Advanced analytics with DuckDB performance
            result = conn.execute(f"""
                WITH job_stats AS (
                    SELECT 
                        application_status,
                        COUNT(*) as count,
                        AVG(CASE WHEN applied_at IS NOT NULL 
                            THEN julianday(applied_at) - julianday(scraped_at) 
                            ELSE NULL END) as avg_time_to_apply
                    FROM sqlite_scan('{db_path}', 'jobs')
                    GROUP BY application_status
                )
                SELECT 
                    json_object(
                        'method', 'DuckDB',
                        'total_jobs', (SELECT SUM(count) FROM job_stats),
                        'status_distribution', json_group_object(application_status, count),
                        'avg_time_to_apply_days', COALESCE(
                            (SELECT avg_time_to_apply FROM job_stats WHERE application_status = 'applied'),
                            0
                        )
                    ) as analytics
            """).fetchone()[0]
            
            import json
            analytics_data = json.loads(result)
            
            return self.success_response(data=analytics_data)

# Global service instance  
analytics_service = AnalyticsService()
```

### Phase 3: AI Processing & Scraping (Week 1, Days 5-6)

**Implements**: ADR-010 (Local AI), ADR-011 (Hybrid Strategy), ADR-013 (Scraping), ADR-014 (Hybrid Scraping)

#### 3.1 AI Processing with LiteLLM

**File**: `config/litellm.yaml`

```yaml
# LiteLLM configuration for hybrid AI strategy (ADR-010, ADR-011)
model_list:
  - model_name: qwen-local
    litellm_params:
      model: openai/Qwen/Qwen2.5-3B-Instruct
      base_url: http://localhost:8080/v1
      api_key: "not-needed"
      
  - model_name: gpt-4o-mini-fallback
    litellm_params:
      model: gpt-4o-mini
      api_key: ${OPENAI_API_KEY}

router_settings:
  routing_strategy: usage-based-routing
  fallback_models:
    - gpt-4o-mini-fallback
  timeout: 30
  retry_after: 10
```

**File**: `src/services/ai_service.py`

```python
from typing import Dict, Any, Optional
import tiktoken
from litellm import completion
from instructor import from_litellm
from pydantic import BaseModel, Field
from src.services.base_service import BaseService
import streamlit as st

class JobExtractionSchema(BaseModel):
    """Structured job data extraction schema (ADR-010)"""
    title: str = Field(description="Job title")
    company: str = Field(description="Company name")
    location: Optional[str] = Field(description="Job location")
    salary_min: Optional[int] = Field(description="Minimum salary")
    salary_max: Optional[int] = Field(description="Maximum salary")
    remote: bool = Field(description="Remote work available")
    description: str = Field(description="Job description")
    requirements: Optional[str] = Field(description="Job requirements")

class AIService(BaseService):
    """AI processing with hybrid local/cloud strategy (ADR-011)"""
    
    def __init__(self):
        self.token_threshold = 8000  # From ADR-012
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.instructor_client = from_litellm(completion)
    
    def extract_job_data(self, raw_content: str) -> Dict[str, Any]:
        """Extract structured job data with intelligent routing"""
        try:
            # Token counting for routing decision (ADR-012)
            token_count = len(self.tokenizer.encode(raw_content))
            
            # Route to local vs cloud based on token count
            if token_count <= self.token_threshold:
                model = "qwen-local"
                st.info(f"Processing with local AI ({token_count} tokens)")
            else:
                model = "gpt-4o-mini-fallback" 
                st.info(f"Processing with cloud AI ({token_count} tokens)")
            
            # Extract structured data with Instructor validation
            job_data = self.instructor_client.chat.completions.create(
                model=model,
                response_model=JobExtractionSchema,
                messages=[
                    {
                        "role": "system",
                        "content": "Extract structured job information from the provided content. Be precise and extract only factual information."
                    },
                    {
                        "role": "user", 
                        "content": f"Extract job data from:\n\n{raw_content[:2000]}"
                    }
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            return self.success_response(
                data=job_data.dict(),
                message=f"Extracted job data using {model}"
            )
            
        except Exception as e:
            return self.handle_service_error("extract_job_data", e)

# Global service instance
ai_service = AIService()
```

#### 3.2 2-Tier Scraping Implementation

**File**: `src/services/scraping_service.py`

```python
from typing import Dict, Any, List
import asyncio
import httpx
from jobspy import scrape_jobs
from scrapegraphai import SmartScraperGraph
from src.services.base_service import BaseService
from src.services.ai_service import ai_service
import streamlit as st

class ScrapingService(BaseService):
    """2-tier scraping with JobSpy and ScrapeGraphAI (ADR-013, ADR-014)"""
    
    def __init__(self):
        # IPRoyal proxy configuration (ADR-015)
        self.proxy_config = {
            "http://": "http://user:pass@proxy.iproyal.com:12321",
            "https://": "http://user:pass@proxy.iproyal.com:12321"
        }
        
        # ScrapeGraphAI configuration for Tier 2
        self.graph_config = {
            "llm": {
                "model": "gpt-4o-mini",
                "openai_api_key": "your-key-here"
            }
        }
    
    async def scrape_job_boards(self, search_terms: List[str], 
                              max_results: int = 50) -> Dict[str, Any]:
        """Tier 1: Structured job board scraping with JobSpy"""
        try:
            with st.status("Scraping job boards...", expanded=True) as status:
                all_jobs = []
                
                for term in search_terms:
                    status.write(f"Searching for: {term}")
                    
                    # JobSpy with proxy support (ADR-013)
                    jobs = scrape_jobs(
                        search_term=term,
                        location="Remote",
                        results_wanted=max_results // len(search_terms),
                        hours_old=72,
                        country_indeed="USA",
                        # Proxy configuration from ADR-015
                        proxies=self.proxy_config if st.secrets.get("USE_PROXIES") else None
                    )
                    
                    all_jobs.extend(jobs.to_dict('records'))
                    status.write(f"Found {len(jobs)} jobs for {term}")
                
                status.update(label="Job board scraping complete!", state="complete")
                
                return self.success_response(
                    data=all_jobs,
                    message=f"Scraped {len(all_jobs)} jobs from job boards"
                )
                
        except Exception as e:
            return self.handle_service_error("scrape_job_boards", e)
    
    async def scrape_company_page(self, company_url: str) -> Dict[str, Any]:
        """Tier 2: AI-powered company page scraping with ScrapeGraphAI"""
        try:
            # ScrapeGraphAI for complex page extraction (ADR-014)
            smart_scraper = SmartScraperGraph(
                prompt="Extract all job postings with title, location, and description",
                source=company_url,
                config=self.graph_config
            )
            
            with st.status(f"AI scraping: {company_url}", expanded=False) as status:
                raw_result = smart_scraper.run()
                status.write("Raw extraction complete, processing with AI...")
                
                # Process with AI service for structured extraction
                processed_result = ai_service.extract_job_data(str(raw_result))
                
                status.update(label="Company page scraping complete!", state="complete")
                
                return self.success_response(
                    data=processed_result.get("data", {}),
                    message=f"AI-scraped company page: {company_url}"
                )
                
        except Exception as e:
            return self.handle_service_error("scrape_company_page", e)
    
    async def resilient_scrape(self, targets: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Resilient scraping with native HTTPX retries (ADR-016)"""
        results = []
        
        # HTTPX client with built-in resilience (ADR-016)
        async with httpx.AsyncClient(
            proxies=self.proxy_config if st.secrets.get("USE_PROXIES") else None,
            timeout=30.0,
            limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
            transport=httpx.HTTPTransport(retries=3)  # Native HTTPX retries
        ) as client:
            
            semaphore = asyncio.Semaphore(3)  # Limit concurrent requests
            
            async def scrape_single(target):
                async with semaphore:
                    try:
                        if target["type"] == "job_board":
                            return await self.scrape_job_boards(target["search_terms"])
                        elif target["type"] == "company_page":
                            return await self.scrape_company_page(target["url"])
                    except Exception as e:
                        st.error(f"Failed to scrape {target}: {e}")
                        return self.handle_service_error("scrape_single", e)
            
            # Execute all scraping tasks
            tasks = [scrape_single(target) for target in targets]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            return self.success_response(
                data=results,
                message=f"Completed {len(results)} scraping tasks"
            )

# Global service instance
scraping_service = ScrapingService()
```

### Phase 4: Modern UI & Complete Integration (Week 1, Day 7)

**Implements**: ADR-021 (Modern Job Cards), ADR-020 (Status Tracking), Complete integration

#### 4.1 Modern Job Cards UI

**File**: `src/ui/components/job_cards.py`

```python
import streamlit as st
from typing import List, Dict, Any
from src.services.search_service import search_service
from src.services.analytics_service import analytics_service
from src.models.database import ApplicationStatus

class JobCardRenderer:
    """Modern card-based job display (ADR-021)"""
    
    def __init__(self):
        self.status_colors = {
            "new": "#E3F2FD",
            "interested": "#FFF3E0",
            "applied": "#E8F5E8", 
            "rejected": "#FFEBEE"
        }
        self.status_icons = {
            "new": "üÜï",
            "interested": "‚≠ê",
            "applied": "‚úÖ",
            "rejected": "‚ùå"
        }
    
    def render_job_browser(self):
        """Complete job browser with search and cards"""
        st.title("üéØ Job Browser")
        
        # Search interface
        col1, col2 = st.columns([3, 1])
        
        with col1:
            search_query = st.text_input(
                "Search jobs",
                placeholder="python developer, remote, machine learning...",
                key="job_search"
            )
        
        with col2:
            status_filter = st.multiselect(
                "Status Filter",
                ["new", "interested", "applied", "rejected"],
                default=["new", "interested"],
                key="status_filter"
            )
        
        # Execute search or show recent jobs
        if search_query:
            status_enums = [ApplicationStatus(s) for s in status_filter]
            jobs = search_service.search_jobs(search_query, status_enums)
            st.success(f"Found {len(jobs)} jobs matching '{search_query}'")
        else:
            jobs = search_service.get_recent_jobs(50)
            if status_filter:
                jobs = [j for j in jobs if j.get('application_status', 'new') in status_filter]
            st.info(f"Showing {len(jobs)} recent jobs")
        
        # Render job cards
        if jobs:
            self.render_job_cards(jobs)
        else:
            st.info("No jobs found. Try adjusting your search or filters.")
    
    def render_job_cards(self, jobs: List[Dict[str, Any]]):
        """Render jobs as responsive cards"""
        # Mobile-responsive columns
        cols_per_row = 3 if len(jobs) >= 3 else len(jobs)
        
        # Pagination for performance
        items_per_page = 12
        total_pages = (len(jobs) + items_per_page - 1) // items_per_page
        
        if total_pages > 1:
            page = st.selectbox("Page", range(1, total_pages + 1), key="card_page")
            start_idx = (page - 1) * items_per_page
            jobs_page = jobs[start_idx:start_idx + items_per_page]
        else:
            jobs_page = jobs
        
        # Render cards in grid
        for i in range(0, len(jobs_page), cols_per_row):
            row_jobs = jobs_page[i:i + cols_per_row]
            cols = st.columns(len(row_jobs))
            
            for col, job in zip(cols, row_jobs):
                with col:
                    self.render_job_card(job)
    
    def render_job_card(self, job: Dict[str, Any]):
        """Render individual job card"""
        status = job.get('application_status', 'new')
        card_color = self.status_colors.get(status, "#FFFFFF")
        
        # Card container with styling
        with st.container():
            st.markdown(f"""
                <div style="
                    background-color: {card_color};
                    border: 1px solid #E0E0E0;
                    border-radius: 12px;
                    padding: 16px;
                    margin-bottom: 16px;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                ">
            """, unsafe_allow_html=True)
            
            # Header: Company and Title
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown(f"**{job['title']}**")
                st.caption(f"üè¢ {job['company']}")
            with col2:
                icon = self.status_icons.get(status, "")
                st.markdown(f"<div style='text-align: right; font-size: 1.2em;'>{icon}</div>", 
                           unsafe_allow_html=True)
            
            # Body: Key details
            if job.get('location'):
                st.caption(f"üìç {job['location']}")
            
            if job.get('salary_min'):
                st.metric("Salary", f"${job['salary_min']:,}+")
            
            # Footer: Actions
            col1, col2 = st.columns([2, 1])
            
            with col1:
                if job.get('url'):
                    st.link_button("Apply", job['url'], use_container_width=True)
            
            with col2:
                # Status update
                new_status = st.selectbox(
                    "Status",
                    ["new", "interested", "applied", "rejected"],
                    index=["new", "interested", "applied", "rejected"].index(status),
                    key=f"status_{job['id']}",
                    label_visibility="collapsed"
                )
                
                # Handle status change
                if new_status != status:
                    # Would integrate with JobStatusService here
                    st.success(f"Status updated to {new_status}")
                    st.rerun()
            
            st.markdown("</div>", unsafe_allow_html=True)

# Global card renderer
card_renderer = JobCardRenderer()
```

#### 4.2 Main Application Integration

**File**: `streamlit_app.py`

```python
import streamlit as st
from src.ui.components.job_cards import card_renderer
from src.services.analytics_service import analytics_service
from src.services.scraping_service import scraping_service

# Page configuration (ADR-021)
st.set_page_config(
    page_title="AI Job Scraper",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="collapsed"
)

def main():
    """Main application with navigation"""
    
    # Sidebar navigation
    with st.sidebar:
        st.title("üéØ AI Job Scraper")
        page = st.radio(
            "Navigation",
            ["Job Browser", "Analytics", "Scraping", "Settings"]
        )
    
    # Page routing
    if page == "Job Browser":
        card_renderer.render_job_browser()
        
    elif page == "Analytics":
        render_analytics_dashboard()
        
    elif page == "Scraping":
        render_scraping_interface()
        
    elif page == "Settings":
        render_settings()

def render_analytics_dashboard():
    """Analytics dashboard with intelligent method selection"""
    st.title("üìä Analytics Dashboard")
    
    # Get analytics with automatic method selection
    analytics_result = analytics_service.get_job_analytics()
    
    if analytics_result["success"]:
        data = analytics_result["data"]
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Jobs", data["total_jobs"])
        
        with col2:
            app_rate = data.get("application_rate_percent", 0)
            st.metric("Application Rate", f"{app_rate}%")
        
        with col3:
            rej_rate = data.get("rejection_rate_percent", 0)
            st.metric("Rejection Rate", f"{rej_rate}%")
        
        with col4:
            method = data.get("method", "SQLite")
            st.metric("Analytics Engine", method)
        
        # Status distribution chart
        if "status_distribution" in data:
            import plotly.express as px
            
            statuses = list(data["status_distribution"].keys())
            counts = list(data["status_distribution"].values())
            
            fig = px.pie(
                values=counts,
                names=[s.title() for s in statuses],
                title="Application Status Distribution"
            )
            st.plotly_chart(fig, use_container_width=True)
    
    else:
        st.error(f"Analytics error: {analytics_result['error']}")

def render_scraping_interface():
    """Scraping interface with 2-tier strategy"""
    st.title("üï∑Ô∏è Job Scraping")
    
    tab1, tab2 = st.tabs(["Job Boards", "Company Pages"])
    
    with tab1:
        st.subheader("Tier 1: Job Board Scraping")
        
        search_terms = st.text_area(
            "Search Terms (one per line)",
            value="Python Developer\nMachine Learning Engineer\nRemote Software Engineer"
        ).strip().split('\n')
        
        max_results = st.number_input("Max Results", min_value=10, max_value=200, value=50)
        
        if st.button("Start Job Board Scraping", type="primary"):
            result = scraping_service.scrape_job_boards(search_terms, max_results)
            
            if result["success"]:
                st.success(result["message"])
                st.json(result["data"][:3])  # Show first 3 results
            else:
                st.error(result["error"])
    
    with tab2:
        st.subheader("Tier 2: Company Page Scraping")
        
        company_url = st.text_input("Company Career Page URL")
        
        if st.button("Start AI Scraping", type="primary") and company_url:
            result = scraping_service.scrape_company_page(company_url)
            
            if result["success"]:
                st.success(result["message"])
                st.json(result["data"])
            else:
                st.error(result["error"])

def render_settings():
    """Application settings"""
    st.title("‚öôÔ∏è Settings")
    
    with st.expander("AI Configuration"):
        st.write("**Token Threshold**: 8000 tokens (local vs cloud routing)")
        st.write("**Local Model**: Qwen3-4B-Instruct")
        st.write("**Cloud Fallback**: GPT-4o-mini")
    
    with st.expander("Proxy Configuration"):
        use_proxies = st.checkbox("Enable Proxy Usage", value=False)
        if use_proxies:
            st.info("Proxies will be used for scraping to avoid rate limits")
    
    with st.expander("Database Status"):
        st.write("**Database**: SQLite with FTS5 search")
        st.write("**Analytics**: Intelligent SQLite/DuckDB selection")

if __name__ == "__main__":
    main()
```

## Development Environment Setup (ADR-023)

### Docker Development Environment

**File**: `docker-compose.yml`

```yaml
version: '3.8'

services:
  ai-job-scraper:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: ai-job-scraper-dev
    ports:
      - "8501:8501"  # Streamlit default port
    volumes:
      - ./src:/app/src          # Hot reload
      - ./data:/app/data        # Database persistence
      - ./.env:/app/.env        # Environment config
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=sqlite:///./data/jobs.db
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'

volumes:
  app_data:
```

**File**: `Dockerfile`

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv for dependency management
RUN pip install uv

# Copy dependency files
COPY pyproject.toml ./
COPY uv.lock* ./

# Install dependencies
RUN uv sync

# Copy application code
COPY . .

# Create data directory
RUN mkdir -p data

# Expose Streamlit port
EXPOSE 8501

# Development command
CMD ["uv", "run", "streamlit", "run", "streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

### Development Scripts

**File**: `scripts/dev-start.sh`

```bash
#!/bin/bash
echo "Starting AI Job Scraper development environment..."

# Create necessary directories
mkdir -p data logs

# Initialize database if it doesn't exist
if [ ! -f "data/jobs.db" ]; then
    echo "Initializing database..."
    uv run python src/database/init_database.py
fi

# Start Docker environment
docker-compose up --build -d

echo "‚úÖ Development environment started!"
echo "   Frontend: http://localhost:8501"
echo "   Logs: docker-compose logs -f"
```

## Testing Strategy (ADR-004)

### Core Test Structure

**File**: `tests/test_services.py`

```python
import pytest
from src.services.job_service import job_service
from src.services.search_service import search_service
from src.services.analytics_service import analytics_service

def test_job_service_creation():
    """Test job creation with validation"""
    job_data = {
        "title": "Python Developer",
        "company": "Test Corp",
        "url": "https://test.com/job/123"
    }
    
    result = job_service.create_job(job_data)
    assert result["success"] == True
    assert result["data"]["title"] == "Python Developer"

def test_search_service_basic():
    """Test basic FTS5 search functionality"""
    results = search_service.search_jobs("python developer")
    assert isinstance(results, list)

def test_analytics_service_method_selection():
    """Test intelligent method selection for analytics"""
    result = analytics_service.get_job_analytics()
    assert result["success"] == True
    assert "method" in result["data"]  # Should indicate SQLite or DuckDB

@pytest.mark.asyncio
async def test_scraping_resilience():
    """Test scraping resilience patterns"""
    # Test would verify HTTPX retry behavior
    pass
```

**File**: `tests/test_integration.py`

```python
import pytest
from src.services.ai_service import ai_service

def test_ai_token_routing():
    """Test 8000-token threshold routing"""
    short_content = "Python developer position at tech company."
    long_content = "A" * 10000  # Exceeds 8000 token threshold
    
    # Short content should use local model
    result_short = ai_service.extract_job_data(short_content)
    # Long content should use cloud fallback
    result_long = ai_service.extract_job_data(long_content)
    
    # Both should succeed but use different models
    assert result_short["success"] == True
    assert result_long["success"] == True

def test_status_tracking_integration():
    """Test application status tracking with UI integration"""
    # Test would verify status enum patterns work correctly
    pass
```

## Deployment Checklist

### Pre-Deployment Verification

```bash
# 1. Run all tests
uv run pytest tests/ -v

# 2. Check code formatting
uv run ruff format .
uv run ruff check . --fix

# 3. Verify database initialization
uv run python src/database/init_database.py

# 4. Test AI service configuration
uv run python -c "from src.services.ai_service import ai_service; print('AI service ready')"

# 5. Verify Docker build
docker-compose build

# 6. Test complete application
docker-compose up -d
curl http://localhost:8501  # Should return Streamlit interface
```

### Production Configuration

**File**: `.env.production`

```bash
# Production environment configuration
ENVIRONMENT=production
DATABASE_URL=sqlite:///./data/jobs.db

# AI Processing
OPENAI_API_KEY=your_production_key_here
AI_TOKEN_THRESHOLD=8000

# Proxy Configuration (IPRoyal)
USE_PROXIES=true
PROXY_USERNAME=your_username
PROXY_PASSWORD=your_password

# Cost Monitoring
MONTHLY_BUDGET_LIMIT=50.00
COST_ALERT_THRESHOLD=40.00

# Logging
LOG_LEVEL=INFO
```

## Performance Monitoring

### Key Metrics to Track

1. **Search Performance**: <10ms for FTS5 queries
2. **AI Processing**: 98% local processing rate
3. **Cost Monitoring**: Real-time tracking vs $50/month budget
4. **Database Performance**: Query execution times
5. **UI Responsiveness**: Job card rendering <200ms

### Monitoring Dashboard Integration

The analytics service provides real-time performance monitoring with automatic optimization through intelligent method selection between SQLite and DuckDB based on actual performance metrics.

## Troubleshooting Guide

### Common Issues

**Database Connection Issues**:

```bash
# Check database file permissions
ls -la data/jobs.db

# Reinitialize if corrupted
rm data/jobs.db
uv run python src/database/init_database.py
```

**AI Processing Failures**:

- Check token threshold configuration (8000 default)
- Verify LiteLLM configuration in `config/litellm.yaml`
- Test local model availability vs cloud fallback

**Search Performance**:

- FTS5 index status: Check automatic trigger creation
- Analytics method selection: Monitor SQLite vs DuckDB usage

**Docker Issues**:

```bash
# Reset development environment
docker-compose down -v
docker-compose build --no-cache
docker-compose up
```

## Next Steps

After successful Phase 4 deployment:

1. **Monitor Performance**: Track all key metrics and optimize based on actual usage
2. **Cost Optimization**: Fine-tune AI processing thresholds and proxy usage
3. **Feature Enhancement**: Add advanced filters, saved searches, and application tracking
4. **Scaling Preparation**: Monitor for SQLite ‚Üí PostgreSQL transition triggers

The implementation follows the library-first philosophy throughout, achieving 95%+ code reuse and maintaining the zero-maintenance operational goal while delivering a complete job search management platform.
